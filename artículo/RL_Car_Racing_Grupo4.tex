\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% IEEE keywords label in Spanish
\renewcommand{\IEEEkeywordsname}{Palabras clave}

% Spanish and encoding
\usepackage[spanish,es-lcroman]{babel}
% Ensure table captions use "Tabla" (not "Cuadro")
\addto\captionsspanish{\renewcommand{\tablename}{Tabla}}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% Math, tables, figures
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}

% Links
\usepackage{hyperref}
\hypersetup{hidelinks}

\begin{document}

% ===== Portada (formato de entrega) =====
\hypersetup{pageanchor=false}
\begin{titlepage}
    \centering
    {\scshape\Large UNIVERSIDAD NACIONAL DE INGENIERÍA \par}
    {\scshape\large Facultad de Ingeniería Industrial y de Sistemas \par}
    {\scshape\large Unidad de Posgrado \par}
    {\scshape\large Maestría en Inteligencia Artificial \par}
    \vspace{1.0cm}
    \includegraphics[width=0.30\textwidth]{figuras/logo.png}\par\vspace{1cm}

    {\Huge\bfseries Trabajo Final \par}
    \vspace{0.75cm}
    {\large Artículo de Investigación \par}
    \vspace{0.5cm}
    {\large\bfseries Aprendizaje por refuerzo profundo para CarRacing-v3 \\ con Red Neuronal Q Profunda (DQN) y variantes \par}
    \vspace{0.75cm}

    {\large \textbf{Curso}: Aprendizaje por Refuerzo \par}
    \vspace{1.25cm}
    {\large\bfseries Grupo 4: \par}
    \vspace{0.5cm}
    {\large Koc Góngora, Luis Enrique \par}
    {\large Mancilla Antaya, Alex Felipe \par}
    {\large Meléndez García, Herbert Antonio \par}
    {\large Paitán Cano, Dennis Jack \par}
     \vspace{0.75cm}
    {\large \textbf{Docente}: María Tejada Begazo \par}
    \vfill
    {\large 14 de enero de 2026 \par}
\end{titlepage}

\hypersetup{pageanchor=true}

\title{Aprendizaje por refuerzo profundo para CarRacing-v3 con Red Neuronal Q Profunda (DQN) y variantes\thanks{Este trabajo se desarrolla como trabajo final del curso de \textit{Aprendizaje por Refuerzo}, como parte de la maestría en Inteligencia Artifical de la Universidad Nacional de Ingeniería (Perú).}}

\author{
\IEEEauthorblockN{Luis Koc, Alex Mancilla, Herbert Meléndez, Dennis Jack Paitán}
\IEEEauthorblockA{Unidad de Posgrado, Universidad Nacional de Ingeniería (UNI)\\
\texttt{luis.koc@gmail.com\qquad amancillaa@uni.pe}\\
\texttt{hamg.94@gmail.com\qquad dennis.paitan.c@uni.pe}}
}

\maketitle

\begin{abstract}
Se implementa y evalúa un agente de aprendizaje por refuerzo profundo para el entorno visual \textit{CarRacing-v3} de Gymnasium, donde el agente aprende a conducir un vehículo usando únicamente observaciones rojo-verde-azul (RGB) de $96\times 96$ píxeles del entorno de prueba. Se compara el algoritmo DQN con dos extensiones ampliamente adoptadas para estabilizar el aprendizaje y mejorar el desempeño: DQN Doble que reduce la sobreestimación) y DQN con arquitectura \textit{dueling} para descomponer el valor del estado y la ventaja de la acción.

Para adaptar CarRacing que requiere naturalmetne acciones continuas a DQN que solo soporta acciones discretas, se discretiza el control \mbox{(timon, acelerador, freno)} en 12 acciones fijas. El estado se preprocesa con conversión a escala de grises, normalización y apilamiento de 4 frames (frame stacking) para capturar dinámica temporal.

Los resultados se reportan mediante evaluación sin exploración ($\epsilon=0$) en 5 episodios, y se presentan tablas y gráficos de entrenamiento/evaluación generados automáticamente por el proyecto. En las corridas evaluadas, Double DQN alcanzó el mayor retorno promedio a 1000 episodios, mientras que a 200 episodios los retornos fueron más variables, evidenciando sensibilidad a la etapa de entrenamiento y a la varianza del entorno.
\end{abstract}

\begin{IEEEkeywords}
Aprendizaje por refuerzo, DQN, Double DQN, Dueling DQN, CarRacing-v3, Gymnasium, PyTorch.
\end{IEEEkeywords}

\section{Introducción}
El aprendizaje por refuerzo (RL) estudia agentes que aprenden políticas a partir de interacción con un entorno, maximizando una recompensa acumulada \cite{sutton2018reinforcement}. En problemas de control a partir de percepción visual, el uso de redes convolucionales para aproximar funciones de valor permitió escalar RL a entradas de alta dimensión \cite{mnih2015human}.

En este trabajo se utiliza el entorno \textit{CarRacing-v3} provisto por Gymnasium \cite{gymnasium, gymnasium_carracing}. Es un escenario de conducción con observaciones visuales y dinámica física (implementada sobre Box2D \cite{box2d}) cuyo control nativo es continuo. Sobre este entorno se evalúa el impacto de mejoras conocidas sobre DQN ---Double DQN \cite{vanhasselt2016double} y Dueling DQN \cite{wang2016dueling}--- en comparación con el DQN base \cite{mnih2015human}. Finalmente, se prioriza la reproducibilidad mediante el uso de un repositorio público en \textit{Github} .

\textbf{Objetivos:} (i) entrenar agentes DQN y variantes en CarRacing-v3; (ii) registrar métricas y gráficos de entrenamiento; (iii) evaluar los modelos sin exploración y comparar retornos con tablas/figuras reproducibles.

\section{Problema y motivación}
En \textit{CarRacing-v3}, el agente recibe únicamente observaciones visuales y debe tomar decisiones de control para maximizar el retorno acumulado mientras se desplaza por la pista \cite{gymnasium_carracing}. La combinación de percepción de alta dimensión (imágenes), dinámica no lineal del simulador y exploración durante el aprendizaje lo convierte en un banco de prueba representativo para RL profundo \cite{sutton2018reinforcement, mnih2015human}.

La motivación práctica es estudiar la estabilidad del entrenamiento con recursos limitados (ejecución en una unidad central de procesamiento (CPU) y entrenamiento sin renderizado) y comprender qué variante de DQN ofrece mejor compromiso entre complejidad y desempeño, manteniendo una implementación controlada.

\section{Metodología}
\subsection{Entorno}
Se utiliza Gymnasium \cite{gymnasium} con el entorno \textit{CarRacing-v3} \cite{gymnasium_carracing}. Gymnasium es una librería/framework de código abierto en Python para aprendizaje por refuerzo que proporciona una API estándar para interactuar con entornos  y para describir formalmente los espacios de observaciones y acciones. Además, incluye utilidades (
\textit{wrappers}) para transformar observaciones/recompensas y facilitar la experimentación reproducible. La interacción agente--entorno sigue la especificación oficial: la observación es una imagen RGB de $96\times 96$ píxeles y la acción corresponde, en nuestro caso, a un control continuo de conducción: \mbox{(timón, acelerador, freno)} (en inglés: \textit{steer, gas, brake}).

La dinámica de la simulación se implementa sobre Box2D \cite{box2d}, un motor de física 2D usado para simular colisiones y movimiento.

\subsection{Discretización del espacio de acciones}
DQN asume un espacio de acciones discreto \cite{mnih2015human}. Por ello, se define un mapeo desde el control continuo \mbox{(steer, gas, brake)} hacia un conjunto finito de 12 acciones. Cada acción corresponde a una combinación de giro $\in\{-1,0,1\}$, aceleración $\in\{0,1\}$ y freno $\in\{0,0.2\}$. Este esquema busca cubrir maniobras básicas (recto, giros, acelerar y frenar) respetando el formato de control y las convenciones descritas para el entorno \cite{gymnasium_carracing}.

\subsection{Preprocesamiento y estado}
Se aplica un preprocesamiento de imágenes para reducir variabilidad y costo computacional: cada frame se convierte a escala de grises y se reescala a $[0,1]$ usando OpenCV \cite{bradski2000opencv}. Para incorporar información temporal sin recurrir a modelos recurrentes, se apilan los 4 frames más recientes (\textit{frame stacking}), una estrategia habitual en DQN para entradas visuales \cite{mnih2015human}. El estado resultante tiene forma $(4,96,96)$.

\subsection{Modelos: DQN, Double DQN y Dueling DQN}
Se implementa un aproximador $Q_\theta(s,a)$ con una red neuronal convolucional (CNN, por sus siglas en inglés: \textit{Convolutional Neural Network}) sencilla (2 capas convolucionales + un perceptrón multicapa (MLP, por sus siglas en inglés: \textit{Multi-Layer Perceptron})). El entrenamiento usa \textit{experience replay} \cite{lin1992replay} y política $\epsilon$-greedy \cite{sutton2018reinforcement}.

\textbf{DQN:} propuesto por Mnih et al. \cite{mnih2015human}, aprende $Q(s,a)$ con una red profunda y estabiliza el entrenamiento mediante \textit{experience replay} y una red objetivo $Q_{\theta^-}$.\\
\textbf{Double DQN:} propuesto por van Hasselt et al. \cite{vanhasselt2016double}, reduce la sobreestimación al desacoplar selección (red online) y evaluación (red objetivo) de la acción en el \textit{target}.\\
\textbf{Dueling DQN:} propuesto por Wang et al. \cite{wang2016dueling}, usa una arquitectura que separa valor del estado $V(s)$ y ventaja $A(s,a)$ para estimar $Q(s,a)$ con mayor eficiencia.\\
Las variantes se activan con banderas de configuración (Double/Dueling) y pueden combinarse (Dueling Double DQN).

\subsection{Entrenamiento}
El optimizador usado es Adam, propuesto por Kingma y Ba (del acrónimo \textit{Adaptive Moment Estimation}) \cite{kingma2015adam}, que adapta la tasa de aprendizaje por parámetro usando estimaciones del primer y segundo momento del gradiente; suele converger rápido y ser estable, aunque puede ser sensible a la tasa de aprendizaje y no siempre generaliza tan bien como SGD en algunos problemas \cite{wilson2017marginal, reddi2019adam}. Se entrena por episodios, con red objetivo actualizada periódicamente (cada 5 episodios) y guardado de checkpoints. Para acelerar entrenamiento, se aplica \textit{frame skipping} \cite{mnih2015human} con \texttt{SKIP\_FRAMES=2}.

\section{Arquitectura del proyecto}
La Figura~\ref{fig:arquitectura_proyecto} resume la organización del proyecto y el flujo de ejecución. El punto de entrada es \texttt{main.py}, que orquesta (i) la construcción del entorno (Gymnasium/CarRacing-v3) y su preprocesamiento; (ii) la inicialización del agente DQN y sus variantes (Double/Dueling); (iii) el ciclo de entrenamiento (recolección de experiencia, \textit{replay buffer} y actualizaciones de la red); y (iv) la evaluación y el registro de métricas.

La lógica principal se encapsula en el paquete \texttt{src/}: \texttt{entorno.py} concentra la interacción con el entorno y el modo de observación; \texttt{preprocesamiento.py} implementa la preparación del estado (transformaciones de imagen y \textit{frame stacking}); \texttt{agente.py} define el modelo y la política de acción; \texttt{entrenamiento.py} gestiona el bucle de entrenamiento/evaluación; y \texttt{configuracion.py} centraliza hiperparámetros y banderas. Los artefactos de salida (modelos, métricas y gráficas) se guardan en \texttt{resultados/} y se complementan con scripts en \texttt{tests/} para pruebas y evaluación.

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{../diagrama.png}
\caption{Arquitectura del proyecto y flujo de entrenamiento/evaluación del agente DQN en CarRacing-v3.}
\label{fig:arquitectura_proyecto}
\end{figure*}

\section{Diseño experimental}
\subsection{Configuración}
Se entrenan agentes por 1000 episodios (cuando aplica), registrando por episodio: reward acumulado, $\epsilon$, tamaño del buffer y loss promedio. Para la evaluación, se fija $\epsilon=0$ (sin exploración) y se ejecutan 5 episodios por checkpoint. Los retornos por episodio se exportan a un archivo de valores separados por comas (CSV, por sus siglas en inglés: \textit{Comma-Separated Values}) y se generan gráficos automáticamente. El repositorio y el cuaderno de resultados documentan el flujo de entrenamiento y evaluación (Jupyter \cite{kluyver2016jupyter}).

\subsection{Métrica}
La métrica principal es el retorno total por episodio (reward acumulado), reportado como media y desviación estándar poblacional (\emph{ddof}=0) sobre 5 episodios. Este reporte sigue el resumen implementado en el cuaderno final.

\section{Resultados}
\subsection{Resumen cuantitativo}
La Tabla~\ref{tab:eval_200} y la Tabla~\ref{tab:eval_1000} presentan los retornos de evaluación (5 episodios, $\epsilon=0$) para distintos algoritmos. Además de media y dispersión, se indica cuántos episodios alcanzaron 1000 frames (columna \textit{Full}), ya que el entorno puede finalizar antes por condiciones de término o truncamiento \cite{gymnasium_carracing}.

\begin{table}[!t]
\caption{Evaluación a 200 episodios (5 episodios, $\epsilon=0$)}
\label{tab:eval_200}
\centering
\begin{tabular}{lrrrrrr}
\toprule
Algoritmo & Mean & Std & Min & Max & Full & $n$\\
\midrule
DQN & 238.85 & 207.06 & 67.24 & 646.67 & 4 & 5\\
Double DQN & 108.76 & 81.52 & 30.74 & 265.52 & 5 & 5\\
Dueling Double DQN & 262.85 & 174.93 & 46.63 & 463.14 & 4 & 5\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!t]
\caption{Evaluación a 1000 episodios (5 episodios, $\epsilon=0$)}
\label{tab:eval_1000}
\centering
\begin{tabular}{lrrrrrr}
\toprule
Algoritmo & Mean & Std & Min & Max & Full & $n$\\
\midrule
DQN & 418.58 & 209.95 & 252.73 & 825.09 & 5 & 5\\
Double DQN & 567.72 & 257.19 & 278.29 & 883.33 & 5 & 5\\
Dueling Double DQN & 462.46 & 222.57 & 189.76 & 793.47 & 4 & 5\\
\bottomrule
\end{tabular}
\end{table}

En conjunto, los resultados sugieren que el desempeño mejora con el entrenamiento y que Double DQN tiende a ser más competitivo en este entorno cuando se entrena por más episodios: a 1000 episodios obtiene el mayor retorno promedio (567.72) y también el mayor máximo (883.33), lo que es consistente con su objetivo de reducir la sobreestimación al construir los \textit{targets}. En contraste, a 200 episodios las medias son más inestables y la dispersión es alta (p.ej., DQN: Std=207.06), lo que indica que el ranking entre métodos aún no se consolida en etapas tempranas y que la varianza entre episodios es significativa \cite{sutton2018reinforcement}. La columna \textit{Full} muestra que, aun sin exploración ($\epsilon=0$), algunos episodios terminan antes de 1000 frames (truncamiento/término), por lo que el retorno observado refleja tanto la calidad de la política como la frecuencia de episodios completos.

\subsection{Gráficos de entrenamiento}
En los gráficos de entrenamiento se observa la dinámica típica del aprendizaje: el retorno por episodio tiende a aumentar a medida que el agente consolida una política más efectiva, aunque con oscilaciones debidas a exploración y variabilidad del entorno. En paralelo, $\epsilon$ decae (menor exploración) y el \textit{replay buffer} crece hasta estabilizarse, lo que incrementa la reutilización de experiencias; el \textit{loss} suele ser ruidoso y no necesariamente decreciente de forma monótona, por lo que se interpreta como señal de estabilidad de actualización más que como métrica directa de desempeño.
Las Figuras~\ref{fig:train_double_reward}--\ref{fig:train_dueling_double_reward} ilustran la evolución del retorno por episodio durante el entrenamiento. Estas visualizaciones se obtienen a partir de los registros generados por el proyecto y se construyen con Matplotlib \cite{hunter2007matplotlib}.

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{../resultados/experimento_1000ep_double_dqn/grafico_reward.png}
\caption{Entrenamiento: reward por episodio (Double DQN).}
\label{fig:train_double_reward}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{../resultados/experimento_1000ep_dqn/grafico_reward.png}
\caption{Entrenamiento: reward por episodio (DQN).}
\label{fig:train_dqn_reward}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{../resultados/experimento_1000ep_dueling_double_dqn/grafico_reward.png}
\caption{Entrenamiento: reward por episodio (Dueling Double DQN).}
\label{fig:train_dueling_double_reward}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{../resultados/experimento_1000ep_double_dqn/grafico_loss.png}
\caption{Entrenamiento (Double DQN): loss promedio por episodio.}
\label{fig:train_double_loss}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{../resultados/experimento_1000ep_double_dqn/grafico_epsilon.png}
\caption{Entrenamiento (Double DQN): decaimiento de $\epsilon$ (exploración).}
\label{fig:train_double_epsilon}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{../resultados/experimento_1000ep_double_dqn/grafico_buffer.png}
\caption{Entrenamiento (Double DQN): tamaño del replay buffer.}
\label{fig:train_double_buffer}
\end{figure}

\subsection{Gráficos de evaluación}
Los gráficos de evaluación muestran, para cada checkpoint, el retorno obtenido sin exploración ($\epsilon=0$), por lo que reflejan directamente la calidad de la política aprendida en ese punto del entrenamiento. La dispersión entre episodios y la variación entre checkpoints evidencian (i) la varianza propia del entorno y (ii) que el desempeño no crece de forma estrictamente monótona, sino por tramos, con posibles retrocesos. Al comparar métodos, estas curvas ayudan a identificar tendencias (p.ej., retornos consistentemente más altos o más estables), pero deben leerse con cautela debido al tamaño muestral reducido.
Las Figuras~\ref{fig:eval_1000_double}--\ref{fig:eval_200_dueling_double} muestran los retornos obtenidos en evaluación (5 episodios, $\epsilon=0$) para distintos checkpoints. Dado el tamaño muestral reducido, se interpretan como un apoyo descriptivo y no como evidencia concluyente \cite{sutton2018reinforcement}.

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{../resultados/eval_1000_double_dqn/grafico_rewards.png}
\caption{Evaluación (1000): Double DQN.}
\label{fig:eval_1000_double}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{../resultados/eval_1000_dqn/grafico_rewards.png}
\caption{Evaluación (1000): DQN.}
\label{fig:eval_1000_dqn}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{../resultados/eval_200_dueling_double_dqn/grafico_rewards.png}
\caption{Evaluación (200): Dueling Double DQN.}
\label{fig:eval_200_dueling_double}
\end{figure}

\section{Discusión crítica}
Con los resultados disponibles, el mejor desempeño global lo obtiene Double DQN a 1000 episodios (Tabla~\ref{tab:eval_1000}), ya que alcanza el mayor retorno promedio y máximo. Esto es coherente con su mejora principal: desacoplar selección y evaluación de acciones en el \textit{target} para mitigar la sobreestimación y estabilizar el aprendizaje \cite{vanhasselt2016double}. A 200 episodios (Tabla~\ref{tab:eval_200}) el ranking es menos concluyente y la dispersión sigue siendo alta, lo que sugiere que ese régimen corresponde a una etapa temprana donde la política aún está en formación y la varianza por episodio domina \cite{sutton2018reinforcement}.

El ejercicio muestra que 200 episodios es claramente insuficiente para comparar variantes con confianza en este entorno, mientras que 1000 episodios ofrece una señal más estable pero sigue siendo un presupuesto moderado para RL visual, donde el desempeño puede depender fuertemente de la semilla, de los hiperparámetros y del tiempo total de interacción \cite{mnih2015human, sutton2018reinforcement}.

Limitaciones principales: (i) evaluación con pocos episodios por checkpoint ($n=5$) y sin múltiples semillas, lo que limita inferencias estadísticas; (ii) discretización del control continuo, que impone una política restringida (12 acciones) y puede afectar el techo de desempeño; (iii) ausencia de una búsqueda sistemática de hiperparámetros y análisis de sensibilidad; y (iv) recursos de cómputo acotados (CPU), que condicionan la longitud del entrenamiento y el número de corridas.

\section{Especificaciones y restricciones de implementación}
\textbf{Restricciones:} se desactiva el renderizado interactivo para acelerar el entrenamiento (se usa el modo \texttt{rgb\_array} para obtener observaciones); solo CPU como configuración típica; uso de frame skipping y mini-batches para controlar costo computacional.\\
\textbf{Lenguaje y librerías:} Python \cite{python}, Gymnasium \cite{gymnasium}, PyTorch \cite{paszke2019pytorch}, NumPy \cite{harris2020numpy}, Matplotlib \cite{hunter2007matplotlib}, OpenCV \cite{bradski2000opencv}, pandas \cite{mckinney2010pandas}, ImageIO \cite{imageio} y pygame \cite{pygame}.

\section{Reproducibilidad}
El repositorio del proyecto es: \url{https://github.com/alexmancila/Proyect-RL-CarRacing-DQN/tree/main}.\\
Ver Fig.~\ref{fig:arquitectura_proyecto} para una vista general de los módulos y del flujo de ejecución.\\
La configuración base (por defecto) se define en \texttt{src/configuracion.py}. Para replicar los experimentos: (i) instalar dependencias desde \texttt{requirements.txt}; (ii) ejecutar entrenamiento con \texttt{python main.py}; (iii) correr los scripts de evaluación en \texttt{tests/}. El cuaderno final en Jupyter documenta el flujo de ejecución y la ubicación de los artefactos de salida (modelos, métricas y gráficos) \cite{kluyver2016jupyter}.

\section{Conclusiones y trabajo futuro}
Se cumplieron los objetivos planteados: se entrenaron agentes DQN y variantes en \textit{CarRacing-v3}, se registraron métricas y gráficos de entrenamiento, y se evaluaron los modelos sin exploración ($\epsilon=0$) con reportes reproducibles (tablas y figuras generadas por el proyecto). El logro principal es la implementación integrada y evaluable (DQN, Double y Dueling, incluyendo su combinación) sobre un entorno visual con control originalmente continuo, junto con una canalización de preprocesamiento y registro de resultados que permite comparar configuraciones de forma consistente.

Con la evidencia recopilada, Double DQN muestra el mejor desempeño promedio a 1000 episodios, en línea con su diseño para reducir sobreestimación en el \textit{target} \cite{vanhasselt2016double}. No obstante, estos hallazgos deben interpretarse como descriptivos: la evaluación usa pocos episodios ($n=5$) y no incluye múltiples semillas, y además la discretización del control puede limitar el techo de desempeño y afectar comparaciones.

Como trabajo futuro: (i) aumentar el número de episodios de evaluación y usar múltiples semillas para reducir varianza \cite{sutton2018reinforcement}; (ii) realizar búsqueda y análisis de sensibilidad de hiperparámetros; (iii) incorporar mejoras de muestreo como \textit{prioritized experience replay} \cite{schaul2016prioritized} y probar arquitecturas más expresivas; y (iv) explorar métodos de control continuo para aprovechar la acción nativa del entorno (p.ej., DDPG \cite{lillicrap2016ddpg} o SAC \cite{haarnoja2018sac}).

\section{Participación y contribuciones}
\textbf{Koc Góngora, Luis Enrique:} revisión del pipeline RL y código, análisis de resultados, documentación técnica, revisión bibliográfica, elaboración del informe.\\

\textbf{Mancilla Antaya, Alex Felipe:} integración y ajustes del pipeline, implementación de variantes (Double/Dueling), codificación e información técnica, repositorio Github, soporte en evaluación y automatización,  revisión bibliográfica, revisión del informe y presentación.\\

\textbf{Meléndez García, Herbert Antonio:} revisión del pipeline RL y  código, revisión bibliográfica, revisión del informe, elaboración de presentación y consolidación de resultados/figuras.\\

\textbf{Paitán Cano, Dennis Jack:} revisión del pipeline RL y  código, revisión bibliográfica, revisión del informe, elaboración de presentación y organización de resultados.

\section{Transparencia (uso de herramientas)}
Se utilizó Python \cite{python} para el desarrollo y experimentación, y PyTorch \cite{paszke2019pytorch} como framework principal para implementar y entrenar los modelos de aprendizaje por refuerzo profundo. El análisis y reporte de resultados se realizó con Jupyter Notebook \cite{kluyver2016jupyter}, y la elaboración del informe se realizó en LaTeX \cite{latex}. Para apoyo en redacción y consistencia del documento se empleó asistencia de Inteligencia Artificial (IA) generativa como herramienta de productividad. Las decisiones técnicas, experimentos y resultados reportados provienen del código y documentación del repositorio.

\bibliographystyle{IEEEtran}
\bibliography{rl_references}

\end{document}